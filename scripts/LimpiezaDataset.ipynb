{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PASO 1: Cargar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'archive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#En este paso, cargamos el archivo CSV con los datos de trÃ¡fico de red. \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marchive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'archive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'"
     ]
    }
   ],
   "source": [
    "#En este paso, cargamos el archivo CSV con los datos de trÃ¡fico de red. \n",
    "\n",
    "df = pd.read_csv(\"../archive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detalles del dataset\n",
    "Estamos cargando un CSV que contiene una lista de logs en los que hay posibles ataques DDos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Primeras filas del dataset:\n",
      "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
      "0              54865               3                   2   \n",
      "1              55054             109                   1   \n",
      "2              55055              52                   1   \n",
      "3              46236              34                   1   \n",
      "4              54863               3                   2   \n",
      "\n",
      "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
      "0                        0                           12   \n",
      "1                        1                            6   \n",
      "2                        1                            6   \n",
      "3                        1                            6   \n",
      "4                        0                           12   \n",
      "\n",
      "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
      "0                             0                       6   \n",
      "1                             6                       6   \n",
      "2                             6                       6   \n",
      "3                             6                       6   \n",
      "4                             0                       6   \n",
      "\n",
      "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
      "0                       6                      6.0                     0.0   \n",
      "1                       6                      6.0                     0.0   \n",
      "2                       6                      6.0                     0.0   \n",
      "3                       6                      6.0                     0.0   \n",
      "4                       6                      6.0                     0.0   \n",
      "\n",
      "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
      "0  ...                     20          0.0          0.0            0   \n",
      "1  ...                     20          0.0          0.0            0   \n",
      "2  ...                     20          0.0          0.0            0   \n",
      "3  ...                     20          0.0          0.0            0   \n",
      "4  ...                     20          0.0          0.0            0   \n",
      "\n",
      "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
      "0            0        0.0        0.0          0          0  BENIGN  \n",
      "1            0        0.0        0.0          0          0  BENIGN  \n",
      "2            0        0.0        0.0          0          0  BENIGN  \n",
      "3            0        0.0        0.0          0          0  BENIGN  \n",
      "4            0        0.0        0.0          0          0  BENIGN  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "# TambiÃ©n mostramos las primeras filas del dataset para entender y previsualizar su contenido.\n",
    "print(\"\\n### Primeras filas del dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de las Columnas del Dataset CIC-IDS2017\n",
    "\n",
    "Este dataset contiene mÃºltiples columnas con informaciÃ³n detallada sobre el trÃ¡fico de red. A continuaciÃ³n, se describen las mÃ¡s relevantes:\n",
    "\n",
    "## ð¹ 1. InformaciÃ³n sobre el Flujo de Red  \n",
    "- **Destination Port**: Puerto de destino del trÃ¡fico.  \n",
    "- **Flow Duration**: DuraciÃ³n total del flujo en milisegundos.  \n",
    "- **Total Fwd Packets**: NÃºmero total de paquetes enviados en direcciÃ³n forward.  \n",
    "- **Total Backward Packets**: NÃºmero total de paquetes enviados en direcciÃ³n backward.  \n",
    "\n",
    "## ð¹ 2. CaracterÃ­sticas de los Paquetes  \n",
    "- **Total Length of Fwd Packets**: Longitud total de los paquetes enviados en direcciÃ³n forward.  \n",
    "- **Total Length of Bwd Packets**: Longitud total de los paquetes enviados en direcciÃ³n backward.  \n",
    "- **Fwd Packet Length Max / Min / Mean / Std**: Medidas estadÃ­sticas de la longitud de los paquetes enviados en forward.  \n",
    "- **Bwd Packet Length Max / Min / Mean / Std**: Medidas estadÃ­sticas de la longitud de los paquetes enviados en backward.  \n",
    "\n",
    "## ð¹ 3. EstadÃ­sticas de Tasa de Flujo  \n",
    "- **Flow Bytes/s**: NÃºmero de bytes transmitidos por segundo en el flujo.  \n",
    "- **Flow Packets/s**: NÃºmero de paquetes transmitidos por segundo.  \n",
    "- **Flow IAT Mean / Max / Min / Std**: Intervalo de tiempo promedio, mÃ¡ximo, mÃ­nimo y desviaciÃ³n estÃ¡ndar entre paquetes en el flujo.  \n",
    "\n",
    "## ð¹ 4. InformaciÃ³n sobre Flags y SeÃ±ales de Control  \n",
    "- **SYN Flag Count / FIN Flag Count / RST Flag Count**: Contadores de los flags TCP utilizados en el flujo.  \n",
    "- **PSH Flag Count / ACK Flag Count / URG Flag Count**: Contadores de otros flags de control TCP.  \n",
    "\n",
    "## ð¹ 5. InformaciÃ³n sobre la Ventana TCP  \n",
    "- **Init_Win_bytes_forward**: TamaÃ±o de la ventana TCP en la direcciÃ³n forward.  \n",
    "- **Init_Win_bytes_backward**: TamaÃ±o de la ventana TCP en la direcciÃ³n backward.  \n",
    "\n",
    "## ð¹ 6. Estado del TrÃ¡fico (Label)  \n",
    "- **Label**: Indica si el trÃ¡fico es `BENIGN` (normal) o pertenece a un ataque especÃ­fico como `DDoS`, `PortScan`, etc.  \n",
    "\n",
    "---\n",
    "**Estas caracterÃ­sticas se utilizarÃ¡n para entrenar un modelo de IA que pueda detectar patrones de trÃ¡fico malicioso en redes.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos en el dataset:\n",
      "Flow Bytes/s    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Revisamos cuÃ¡ntos valores nulos hay en cada columna.\n",
    "print(\"\\n### Valores nulos en el dataset:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretacion de resultados:**\n",
    "Vemos que solo hay 4 valores nulos en el dataset, por tanto como son pocos valores nulos vamos a eliminarlos del dataset. Si fuesen muchos valores Nan podrÃ­amos reemplazarlos por la media de la columna para no borrar mucha informaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos en el dataset:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos las filas que contienen valores nulos.\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#comprobamos que no haya valores nulos\n",
    "print(\"\\n### Valores nulos en el dataset:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "df.columns = df.columns.str.strip()  # Elimina espacios en los nombres de columnas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EliminaciÃ³n de Datos Duplicados\n",
    "\n",
    "En el preprocesamiento de datos, es fundamental eliminar los registros duplicados, ya que pueden afectar el rendimiento del modelo de aprendizaje automÃ¡tico. Los duplicados pueden surgir debido a errores en la recopilaciÃ³n de datos o repeticiones en la generaciÃ³n del dataset.\n",
    "\n",
    "## Â¿Por quÃ© eliminamos los duplicados?\n",
    "\n",
    "- **Evita sesgos en el entrenamiento**: Si una clase tiene mÃ¡s registros duplicados, el modelo puede sobreajustarse a esos datos y generalizar mal en nuevos casos.\n",
    "- **Optimiza el uso de recursos**: Trabajar con datos redundantes aumenta el consumo de memoria y tiempo de cÃ³mputo sin aportar nueva informaciÃ³n Ãºtil.\n",
    "- **Mejora la calidad del dataset**: Un dataset sin duplicados garantiza que cada muestra contribuye con informaciÃ³n Ãºnica al modelo, favoreciendo su capacidad de aprendizaje.\n",
    "\n",
    "En este proceso, identificamos y eliminamos registros duplicados utilizando la funciÃ³n `drop_duplicates()`, asegurando que el dataset final contenga Ãºnicamente datos relevantes y sin repeticiones innecesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Cantidad de datos duplicados antes de eliminarlos: 2633\n",
      "Cantidad de datos duplicados despuÃ©s de eliminarlos: 0\n"
     ]
    }
   ],
   "source": [
    "# ## 4ï¸â£ EliminaciÃ³n de Datos Duplicados\n",
    "# Algunos registros pueden estar duplicados, lo que afectarÃ­a el anÃ¡lisis.\n",
    "print(\"\\n### Cantidad de datos duplicados antes de eliminarlos:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Cantidad de datos duplicados despuÃ©s de eliminarlos:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConversiÃ³n de Datos CategÃ³ricos\n",
    "\n",
    "En los modelos de aprendizaje automÃ¡tico, es necesario que todas las variables sean numÃ©ricas, ya que la mayorÃ­a de los algoritmos no pueden procesar datos en formato de texto. En este dataset, la columna `Label` indica si el trÃ¡fico es benigno o corresponde a un ataque, pero estÃ¡ representada como valores categÃ³ricos en texto (`BENIGN`, `DDoS`, etc.).\n",
    "\n",
    "## Â¿Por quÃ© convertimos los datos categÃ³ricos?\n",
    "\n",
    "- **Compatibilidad con modelos de IA**: La mayorÃ­a de los algoritmos de aprendizaje automÃ¡tico solo funcionan con datos numÃ©ricos.\n",
    "- **EstandarizaciÃ³n del dataset**: Facilita la comparaciÃ³n entre distintas clases dentro del modelo.\n",
    "- **OptimizaciÃ³n del procesamiento**: Representar los valores categÃ³ricos como nÃºmeros reduce el tiempo de cÃ³mputo.\n",
    "\n",
    "Para esta conversiÃ³n, se utiliza `LabelEncoder()`, que asigna un nÃºmero a cada categorÃ­a, permitiendo que el modelo pueda interpretar correctamente la variable `Label` sin perder informaciÃ³n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Columna 'Label' despuÃ©s de la conversiÃ³n:\n",
      "Label\n",
      "1    128016\n",
      "0     95092\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ## 5ï¸â£ ConversiÃ³n de Datos CategÃ³ricos\n",
    "# La columna 'Label' indica si el trÃ¡fico es benigno o un ataque. Convertimos los valores de texto a nÃºmeros.\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "\n",
    "#Comprobamos que se haya hecho la conversiÃ³n\n",
    "print(\"\\n### Columna 'Label' despuÃ©s de la conversiÃ³n:\")\n",
    "print(df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NormalizaciÃ³n de Datos NumÃ©ricos\n",
    "\n",
    "En el preprocesamiento de datos, es fundamental escalar las variables numÃ©ricas para mejorar el rendimiento del modelo de aprendizaje automÃ¡tico. La normalizaciÃ³n ayuda a que los modelos sean mÃ¡s estables y precisos, especialmente en algoritmos sensibles a la escala de los datos, como redes neuronales y SVM.\n",
    "\n",
    "## Â¿Por quÃ© normalizamos los datos?\n",
    "\n",
    "- **Diferentes escalas pueden afectar el modelo**: Algunas caracterÃ­sticas tienen valores muy grandes (ej. `Flow Bytes/s`), mientras que otras tienen valores pequeÃ±os (`Packet Length`). Si no se normalizan, el modelo podrÃ­a dar mÃ¡s importancia a ciertas variables solo por su magnitud.\n",
    "- **Mejora la convergencia del entrenamiento**: Los modelos basados en gradiente (como redes neuronales) entrenan mÃ¡s rÃ¡pido y de manera mÃ¡s estable con datos normalizados.\n",
    "- **Evita sesgos en la clasificaciÃ³n**: Al escalar todas las variables a una misma escala, evitamos que unas caracterÃ­sticas dominen sobre otras.\n",
    "\n",
    "## Â¿CÃ³mo lo hacemos?\n",
    "\n",
    "Utilizamos `StandardScaler()` de `sklearn.preprocessing`, que transforma los datos para que tengan una media de 0 y desviaciÃ³n estÃ¡ndar de 1. Esto se aplica a todas las columnas numÃ©ricas del dataset.\n",
    "\n",
    "DespuÃ©s de este paso, los datos estarÃ¡n listos para ser utilizados en el modelo de IA sin riesgo de que la escala de los valores afecte negativamente el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12169/2220213856.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.fillna(df.mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos despuÃ©s de corregir infinitos:\n",
      "0\n",
      "\n",
      "### NormalizaciÃ³n completada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ## 6ï¸â£ NormalizaciÃ³n de Datos NumÃ©ricos\n",
    "\n",
    "# Reemplazamos valores infinitos con la media de la columna\n",
    "df.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Verificamos si aÃºn quedan valores infinitos o nulos despuÃ©s del reemplazo\n",
    "print(\"\\n### Valores nulos despuÃ©s de corregir infinitos:\")\n",
    "print(df.isnull().sum().sum())  # DeberÃ­a ser 0\n",
    "\n",
    "# Aplicamos StandardScaler para normalizar los datos numÃ©ricos\n",
    "scaler = StandardScaler()\n",
    "X = df.drop(columns=['Label'])  # Excluimos Label del escalado\n",
    "y = df['Label']\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "df = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)\n",
    "print(\"\\n### NormalizaciÃ³n completada correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset limpio guardado en: archive/cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ## 7ï¸â£ Guardar Dataset Limpio\n",
    "# Finalmente, guardamos el dataset limpio en un nuevo archivo CSV.\n",
    "cleaned_file_path = \"archive/cleaned_dataset.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"\\n### Dataset limpio guardado en: {cleaned_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
