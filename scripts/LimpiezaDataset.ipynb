{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PASO 1: Cargar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'archive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#En este paso, cargamos el archivo CSV con los datos de tr√°fico de red. \u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marchive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'archive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv'"
     ]
    }
   ],
   "source": [
    "#En este paso, cargamos el archivo CSV con los datos de tr√°fico de red. \n",
    "\n",
    "df = pd.read_csv(\"../archive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detalles del dataset\n",
    "Estamos cargando un CSV que contiene una lista de logs en los que hay posibles ataques DDos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Primeras filas del dataset:\n",
      "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
      "0              54865               3                   2   \n",
      "1              55054             109                   1   \n",
      "2              55055              52                   1   \n",
      "3              46236              34                   1   \n",
      "4              54863               3                   2   \n",
      "\n",
      "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
      "0                        0                           12   \n",
      "1                        1                            6   \n",
      "2                        1                            6   \n",
      "3                        1                            6   \n",
      "4                        0                           12   \n",
      "\n",
      "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
      "0                             0                       6   \n",
      "1                             6                       6   \n",
      "2                             6                       6   \n",
      "3                             6                       6   \n",
      "4                             0                       6   \n",
      "\n",
      "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
      "0                       6                      6.0                     0.0   \n",
      "1                       6                      6.0                     0.0   \n",
      "2                       6                      6.0                     0.0   \n",
      "3                       6                      6.0                     0.0   \n",
      "4                       6                      6.0                     0.0   \n",
      "\n",
      "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
      "0  ...                     20          0.0          0.0            0   \n",
      "1  ...                     20          0.0          0.0            0   \n",
      "2  ...                     20          0.0          0.0            0   \n",
      "3  ...                     20          0.0          0.0            0   \n",
      "4  ...                     20          0.0          0.0            0   \n",
      "\n",
      "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
      "0            0        0.0        0.0          0          0  BENIGN  \n",
      "1            0        0.0        0.0          0          0  BENIGN  \n",
      "2            0        0.0        0.0          0          0  BENIGN  \n",
      "3            0        0.0        0.0          0          0  BENIGN  \n",
      "4            0        0.0        0.0          0          0  BENIGN  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tambi√©n mostramos las primeras filas del dataset para entender y previsualizar su contenido.\n",
    "print(\"\\n### Primeras filas del dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de las Columnas del Dataset CIC-IDS2017\n",
    "\n",
    "Este dataset contiene m√∫ltiples columnas con informaci√≥n detallada sobre el tr√°fico de red. A continuaci√≥n, se describen las m√°s relevantes:\n",
    "\n",
    "## üîπ 1. Informaci√≥n sobre el Flujo de Red  \n",
    "- **Destination Port**: Puerto de destino del tr√°fico.  \n",
    "- **Flow Duration**: Duraci√≥n total del flujo en milisegundos.  \n",
    "- **Total Fwd Packets**: N√∫mero total de paquetes enviados en direcci√≥n forward.  \n",
    "- **Total Backward Packets**: N√∫mero total de paquetes enviados en direcci√≥n backward.  \n",
    "\n",
    "## üîπ 2. Caracter√≠sticas de los Paquetes  \n",
    "- **Total Length of Fwd Packets**: Longitud total de los paquetes enviados en direcci√≥n forward.  \n",
    "- **Total Length of Bwd Packets**: Longitud total de los paquetes enviados en direcci√≥n backward.  \n",
    "- **Fwd Packet Length Max / Min / Mean / Std**: Medidas estad√≠sticas de la longitud de los paquetes enviados en forward.  \n",
    "- **Bwd Packet Length Max / Min / Mean / Std**: Medidas estad√≠sticas de la longitud de los paquetes enviados en backward.  \n",
    "\n",
    "## üîπ 3. Estad√≠sticas de Tasa de Flujo  \n",
    "- **Flow Bytes/s**: N√∫mero de bytes transmitidos por segundo en el flujo.  \n",
    "- **Flow Packets/s**: N√∫mero de paquetes transmitidos por segundo.  \n",
    "- **Flow IAT Mean / Max / Min / Std**: Intervalo de tiempo promedio, m√°ximo, m√≠nimo y desviaci√≥n est√°ndar entre paquetes en el flujo.  \n",
    "\n",
    "## üîπ 4. Informaci√≥n sobre Flags y Se√±ales de Control  \n",
    "- **SYN Flag Count / FIN Flag Count / RST Flag Count**: Contadores de los flags TCP utilizados en el flujo.  \n",
    "- **PSH Flag Count / ACK Flag Count / URG Flag Count**: Contadores de otros flags de control TCP.  \n",
    "\n",
    "## üîπ 5. Informaci√≥n sobre la Ventana TCP  \n",
    "- **Init_Win_bytes_forward**: Tama√±o de la ventana TCP en la direcci√≥n forward.  \n",
    "- **Init_Win_bytes_backward**: Tama√±o de la ventana TCP en la direcci√≥n backward.  \n",
    "\n",
    "## üîπ 6. Estado del Tr√°fico (Label)  \n",
    "- **Label**: Indica si el tr√°fico es `BENIGN` (normal) o pertenece a un ataque espec√≠fico como `DDoS`, `PortScan`, etc.  \n",
    "\n",
    "---\n",
    "**Estas caracter√≠sticas se utilizar√°n para entrenar un modelo de IA que pueda detectar patrones de tr√°fico malicioso en redes.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos en el dataset:\n",
      "Flow Bytes/s    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Revisamos cu√°ntos valores nulos hay en cada columna.\n",
    "print(\"\\n### Valores nulos en el dataset:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretacion de resultados:**\n",
    "Vemos que solo hay 4 valores nulos en el dataset, por tanto como son pocos valores nulos vamos a eliminarlos del dataset. Si fuesen muchos valores Nan podr√≠amos reemplazarlos por la media de la columna para no borrar mucha informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos en el dataset:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos las filas que contienen valores nulos.\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#comprobamos que no haya valores nulos\n",
    "print(\"\\n### Valores nulos en el dataset:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "df.columns = df.columns.str.strip()  # Elimina espacios en los nombres de columnas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminaci√≥n de Datos Duplicados\n",
    "\n",
    "En el preprocesamiento de datos, es fundamental eliminar los registros duplicados, ya que pueden afectar el rendimiento del modelo de aprendizaje autom√°tico. Los duplicados pueden surgir debido a errores en la recopilaci√≥n de datos o repeticiones en la generaci√≥n del dataset.\n",
    "\n",
    "## ¬øPor qu√© eliminamos los duplicados?\n",
    "\n",
    "- **Evita sesgos en el entrenamiento**: Si una clase tiene m√°s registros duplicados, el modelo puede sobreajustarse a esos datos y generalizar mal en nuevos casos.\n",
    "- **Optimiza el uso de recursos**: Trabajar con datos redundantes aumenta el consumo de memoria y tiempo de c√≥mputo sin aportar nueva informaci√≥n √∫til.\n",
    "- **Mejora la calidad del dataset**: Un dataset sin duplicados garantiza que cada muestra contribuye con informaci√≥n √∫nica al modelo, favoreciendo su capacidad de aprendizaje.\n",
    "\n",
    "En este proceso, identificamos y eliminamos registros duplicados utilizando la funci√≥n `drop_duplicates()`, asegurando que el dataset final contenga √∫nicamente datos relevantes y sin repeticiones innecesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Cantidad de datos duplicados antes de eliminarlos: 2633\n",
      "Cantidad de datos duplicados despu√©s de eliminarlos: 0\n"
     ]
    }
   ],
   "source": [
    "# ## 4Ô∏è‚É£ Eliminaci√≥n de Datos Duplicados\n",
    "# Algunos registros pueden estar duplicados, lo que afectar√≠a el an√°lisis.\n",
    "print(\"\\n### Cantidad de datos duplicados antes de eliminarlos:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Cantidad de datos duplicados despu√©s de eliminarlos:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversi√≥n de Datos Categ√≥ricos\n",
    "\n",
    "En los modelos de aprendizaje autom√°tico, es necesario que todas las variables sean num√©ricas, ya que la mayor√≠a de los algoritmos no pueden procesar datos en formato de texto. En este dataset, la columna `Label` indica si el tr√°fico es benigno o corresponde a un ataque, pero est√° representada como valores categ√≥ricos en texto (`BENIGN`, `DDoS`, etc.).\n",
    "\n",
    "## ¬øPor qu√© convertimos los datos categ√≥ricos?\n",
    "\n",
    "- **Compatibilidad con modelos de IA**: La mayor√≠a de los algoritmos de aprendizaje autom√°tico solo funcionan con datos num√©ricos.\n",
    "- **Estandarizaci√≥n del dataset**: Facilita la comparaci√≥n entre distintas clases dentro del modelo.\n",
    "- **Optimizaci√≥n del procesamiento**: Representar los valores categ√≥ricos como n√∫meros reduce el tiempo de c√≥mputo.\n",
    "\n",
    "Para esta conversi√≥n, se utiliza `LabelEncoder()`, que asigna un n√∫mero a cada categor√≠a, permitiendo que el modelo pueda interpretar correctamente la variable `Label` sin perder informaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Columna 'Label' despu√©s de la conversi√≥n:\n",
      "Label\n",
      "1    128016\n",
      "0     95092\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ## 5Ô∏è‚É£ Conversi√≥n de Datos Categ√≥ricos\n",
    "# La columna 'Label' indica si el tr√°fico es benigno o un ataque. Convertimos los valores de texto a n√∫meros.\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "\n",
    "#Comprobamos que se haya hecho la conversi√≥n\n",
    "print(\"\\n### Columna 'Label' despu√©s de la conversi√≥n:\")\n",
    "print(df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizaci√≥n de Datos Num√©ricos\n",
    "\n",
    "En el preprocesamiento de datos, es fundamental escalar las variables num√©ricas para mejorar el rendimiento del modelo de aprendizaje autom√°tico. La normalizaci√≥n ayuda a que los modelos sean m√°s estables y precisos, especialmente en algoritmos sensibles a la escala de los datos, como redes neuronales y SVM.\n",
    "\n",
    "## ¬øPor qu√© normalizamos los datos?\n",
    "\n",
    "- **Diferentes escalas pueden afectar el modelo**: Algunas caracter√≠sticas tienen valores muy grandes (ej. `Flow Bytes/s`), mientras que otras tienen valores peque√±os (`Packet Length`). Si no se normalizan, el modelo podr√≠a dar m√°s importancia a ciertas variables solo por su magnitud.\n",
    "- **Mejora la convergencia del entrenamiento**: Los modelos basados en gradiente (como redes neuronales) entrenan m√°s r√°pido y de manera m√°s estable con datos normalizados.\n",
    "- **Evita sesgos en la clasificaci√≥n**: Al escalar todas las variables a una misma escala, evitamos que unas caracter√≠sticas dominen sobre otras.\n",
    "\n",
    "## ¬øC√≥mo lo hacemos?\n",
    "\n",
    "Utilizamos `StandardScaler()` de `sklearn.preprocessing`, que transforma los datos para que tengan una media de 0 y desviaci√≥n est√°ndar de 1. Esto se aplica a todas las columnas num√©ricas del dataset.\n",
    "\n",
    "Despu√©s de este paso, los datos estar√°n listos para ser utilizados en el modelo de IA sin riesgo de que la escala de los valores afecte negativamente el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12169/2220213856.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.fillna(df.mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos despu√©s de corregir infinitos:\n",
      "0\n",
      "\n",
      "### Normalizaci√≥n completada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ## 6Ô∏è‚É£ Normalizaci√≥n de Datos Num√©ricos\n",
    "\n",
    "# Reemplazamos valores infinitos con la media de la columna\n",
    "df.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Verificamos si a√∫n quedan valores infinitos o nulos despu√©s del reemplazo\n",
    "print(\"\\n### Valores nulos despu√©s de corregir infinitos:\")\n",
    "print(df.isnull().sum().sum())  # Deber√≠a ser 0\n",
    "\n",
    "# Aplicamos StandardScaler para normalizar los datos num√©ricos\n",
    "scaler = StandardScaler()\n",
    "X = df.drop(columns=['Label'])  # Excluimos Label del escalado\n",
    "y = df['Label']\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "df = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)\n",
    "print(\"\\n### Normalizaci√≥n completada correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset limpio guardado en: archive/cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ## 7Ô∏è‚É£ Guardar Dataset Limpio\n",
    "# Finalmente, guardamos el dataset limpio en un nuevo archivo CSV.\n",
    "cleaned_file_path = \"archive/cleaned_dataset.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"\\n### Dataset limpio guardado en: {cleaned_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
