{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PASO 1: Cargar el Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#En este paso, cargamos el archivo CSV con los datos de tr√°fico de red. \n",
    "\n",
    "df = pd.read_csv(\"archive/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detalles del dataset\n",
    "Estamos cargando un CSV que contiene una lista de logs en los que hay posibles ataques DDos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Primeras filas del dataset:\n",
      "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
      "0              54865               3                   2   \n",
      "1              55054             109                   1   \n",
      "2              55055              52                   1   \n",
      "3              46236              34                   1   \n",
      "4              54863               3                   2   \n",
      "\n",
      "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
      "0                        0                           12   \n",
      "1                        1                            6   \n",
      "2                        1                            6   \n",
      "3                        1                            6   \n",
      "4                        0                           12   \n",
      "\n",
      "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
      "0                             0                       6   \n",
      "1                             6                       6   \n",
      "2                             6                       6   \n",
      "3                             6                       6   \n",
      "4                             0                       6   \n",
      "\n",
      "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
      "0                       6                      6.0                     0.0   \n",
      "1                       6                      6.0                     0.0   \n",
      "2                       6                      6.0                     0.0   \n",
      "3                       6                      6.0                     0.0   \n",
      "4                       6                      6.0                     0.0   \n",
      "\n",
      "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
      "0  ...                     20          0.0          0.0            0   \n",
      "1  ...                     20          0.0          0.0            0   \n",
      "2  ...                     20          0.0          0.0            0   \n",
      "3  ...                     20          0.0          0.0            0   \n",
      "4  ...                     20          0.0          0.0            0   \n",
      "\n",
      "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
      "0            0        0.0        0.0          0          0  BENIGN  \n",
      "1            0        0.0        0.0          0          0  BENIGN  \n",
      "2            0        0.0        0.0          0          0  BENIGN  \n",
      "3            0        0.0        0.0          0          0  BENIGN  \n",
      "4            0        0.0        0.0          0          0  BENIGN  \n",
      "\n",
      "[5 rows x 79 columns]\n"
     ]
    }
   ],
   "source": [
    "# Tambi√©n mostramos las primeras filas del dataset para entender y previsualizar su contenido.\n",
    "print(\"\\n### Primeras filas del dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de las Columnas del Dataset CIC-IDS2017\n",
    "\n",
    "Este dataset contiene m√∫ltiples columnas con informaci√≥n detallada sobre el tr√°fico de red. A continuaci√≥n, se describen las m√°s relevantes:\n",
    "\n",
    "## üîπ 1. Informaci√≥n sobre el Flujo de Red  \n",
    "- **Destination Port**: Puerto de destino del tr√°fico.  \n",
    "- **Flow Duration**: Duraci√≥n total del flujo en milisegundos.  \n",
    "- **Total Fwd Packets**: N√∫mero total de paquetes enviados en direcci√≥n forward.  \n",
    "- **Total Backward Packets**: N√∫mero total de paquetes enviados en direcci√≥n backward.  \n",
    "\n",
    "## üîπ 2. Caracter√≠sticas de los Paquetes  \n",
    "- **Total Length of Fwd Packets**: Longitud total de los paquetes enviados en direcci√≥n forward.  \n",
    "- **Total Length of Bwd Packets**: Longitud total de los paquetes enviados en direcci√≥n backward.  \n",
    "- **Fwd Packet Length Max / Min / Mean / Std**: Medidas estad√≠sticas de la longitud de los paquetes enviados en forward.  \n",
    "- **Bwd Packet Length Max / Min / Mean / Std**: Medidas estad√≠sticas de la longitud de los paquetes enviados en backward.  \n",
    "\n",
    "## üîπ 3. Estad√≠sticas de Tasa de Flujo  \n",
    "- **Flow Bytes/s**: N√∫mero de bytes transmitidos por segundo en el flujo.  \n",
    "- **Flow Packets/s**: N√∫mero de paquetes transmitidos por segundo.  \n",
    "- **Flow IAT Mean / Max / Min / Std**: Intervalo de tiempo promedio, m√°ximo, m√≠nimo y desviaci√≥n est√°ndar entre paquetes en el flujo.  \n",
    "\n",
    "## üîπ 4. Informaci√≥n sobre Flags y Se√±ales de Control  \n",
    "- **SYN Flag Count / FIN Flag Count / RST Flag Count**: Contadores de los flags TCP utilizados en el flujo.  \n",
    "- **PSH Flag Count / ACK Flag Count / URG Flag Count**: Contadores de otros flags de control TCP.  \n",
    "\n",
    "## üîπ 5. Informaci√≥n sobre la Ventana TCP  \n",
    "- **Init_Win_bytes_forward**: Tama√±o de la ventana TCP en la direcci√≥n forward.  \n",
    "- **Init_Win_bytes_backward**: Tama√±o de la ventana TCP en la direcci√≥n backward.  \n",
    "\n",
    "## üîπ 6. Estado del Tr√°fico (Label)  \n",
    "- **Label**: Indica si el tr√°fico es `BENIGN` (normal) o pertenece a un ataque espec√≠fico como `DDoS`, `PortScan`, etc.  \n",
    "\n",
    "---\n",
    "**Estas caracter√≠sticas se utilizar√°n para entrenar un modelo de IA que pueda detectar patrones de tr√°fico malicioso en redes.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos en el dataset:\n",
      "Flow Bytes/s    4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Revisamos cu√°ntos valores nulos hay en cada columna.\n",
    "print(\"\\n### Valores nulos en el dataset:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretacion de resultados:**\n",
    "Vemos que solo hay 4 valores nulos en el dataset, por tanto como son pocos valores nulos vamos a eliminarlos del dataset. Si fuesen muchos valores Nan podr√≠amos reemplazarlos por la media de la columna para no borrar mucha informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos en el dataset:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos las filas que contienen valores nulos.\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#comprobamos que no haya valores nulos\n",
    "print(\"\\n### Valores nulos en el dataset:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "df.columns = df.columns.str.strip()  # Elimina espacios en los nombres de columnas\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminaci√≥n de Datos Duplicados\n",
    "\n",
    "En el preprocesamiento de datos, es fundamental eliminar los registros duplicados, ya que pueden afectar el rendimiento del modelo de aprendizaje autom√°tico. Los duplicados pueden surgir debido a errores en la recopilaci√≥n de datos o repeticiones en la generaci√≥n del dataset.\n",
    "\n",
    "## ¬øPor qu√© eliminamos los duplicados?\n",
    "\n",
    "- **Evita sesgos en el entrenamiento**: Si una clase tiene m√°s registros duplicados, el modelo puede sobreajustarse a esos datos y generalizar mal en nuevos casos.\n",
    "- **Optimiza el uso de recursos**: Trabajar con datos redundantes aumenta el consumo de memoria y tiempo de c√≥mputo sin aportar nueva informaci√≥n √∫til.\n",
    "- **Mejora la calidad del dataset**: Un dataset sin duplicados garantiza que cada muestra contribuye con informaci√≥n √∫nica al modelo, favoreciendo su capacidad de aprendizaje.\n",
    "\n",
    "En este proceso, identificamos y eliminamos registros duplicados utilizando la funci√≥n `drop_duplicates()`, asegurando que el dataset final contenga √∫nicamente datos relevantes y sin repeticiones innecesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Cantidad de datos duplicados antes de eliminarlos: 2633\n",
      "Cantidad de datos duplicados despu√©s de eliminarlos: 0\n"
     ]
    }
   ],
   "source": [
    "# ## 4Ô∏è‚É£ Eliminaci√≥n de Datos Duplicados\n",
    "# Algunos registros pueden estar duplicados, lo que afectar√≠a el an√°lisis.\n",
    "print(\"\\n### Cantidad de datos duplicados antes de eliminarlos:\", df.duplicated().sum())\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Cantidad de datos duplicados despu√©s de eliminarlos:\", df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversi√≥n de Datos Categ√≥ricos\n",
    "\n",
    "En los modelos de aprendizaje autom√°tico, es necesario que todas las variables sean num√©ricas, ya que la mayor√≠a de los algoritmos no pueden procesar datos en formato de texto. En este dataset, la columna `Label` indica si el tr√°fico es benigno o corresponde a un ataque, pero est√° representada como valores categ√≥ricos en texto (`BENIGN`, `DDoS`, etc.).\n",
    "\n",
    "## ¬øPor qu√© convertimos los datos categ√≥ricos?\n",
    "\n",
    "- **Compatibilidad con modelos de IA**: La mayor√≠a de los algoritmos de aprendizaje autom√°tico solo funcionan con datos num√©ricos.\n",
    "- **Estandarizaci√≥n del dataset**: Facilita la comparaci√≥n entre distintas clases dentro del modelo.\n",
    "- **Optimizaci√≥n del procesamiento**: Representar los valores categ√≥ricos como n√∫meros reduce el tiempo de c√≥mputo.\n",
    "\n",
    "Para esta conversi√≥n, se utiliza `LabelEncoder()`, que asigna un n√∫mero a cada categor√≠a, permitiendo que el modelo pueda interpretar correctamente la variable `Label` sin perder informaci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Columna 'Label' despu√©s de la conversi√≥n:\n",
      "Label\n",
      "1    128016\n",
      "0     95092\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ## 5Ô∏è‚É£ Conversi√≥n de Datos Categ√≥ricos\n",
    "# La columna 'Label' indica si el tr√°fico es benigno o un ataque. Convertimos los valores de texto a n√∫meros.\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "\n",
    "#Comprobamos que se haya hecho la conversi√≥n\n",
    "print(\"\\n### Columna 'Label' despu√©s de la conversi√≥n:\")\n",
    "print(df['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizaci√≥n de Datos Num√©ricos\n",
    "\n",
    "En el preprocesamiento de datos, es fundamental escalar las variables num√©ricas para mejorar el rendimiento del modelo de aprendizaje autom√°tico. La normalizaci√≥n ayuda a que los modelos sean m√°s estables y precisos, especialmente en algoritmos sensibles a la escala de los datos, como redes neuronales y SVM.\n",
    "\n",
    "## ¬øPor qu√© normalizamos los datos?\n",
    "\n",
    "- **Diferentes escalas pueden afectar el modelo**: Algunas caracter√≠sticas tienen valores muy grandes (ej. `Flow Bytes/s`), mientras que otras tienen valores peque√±os (`Packet Length`). Si no se normalizan, el modelo podr√≠a dar m√°s importancia a ciertas variables solo por su magnitud.\n",
    "- **Mejora la convergencia del entrenamiento**: Los modelos basados en gradiente (como redes neuronales) entrenan m√°s r√°pido y de manera m√°s estable con datos normalizados.\n",
    "- **Evita sesgos en la clasificaci√≥n**: Al escalar todas las variables a una misma escala, evitamos que unas caracter√≠sticas dominen sobre otras.\n",
    "\n",
    "## ¬øC√≥mo lo hacemos?\n",
    "\n",
    "Utilizamos `StandardScaler()` de `sklearn.preprocessing`, que transforma los datos para que tengan una media de 0 y desviaci√≥n est√°ndar de 1. Esto se aplica a todas las columnas num√©ricas del dataset.\n",
    "\n",
    "Despu√©s de este paso, los datos estar√°n listos para ser utilizados en el modelo de IA sin riesgo de que la escala de los valores afecte negativamente el entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12169/2220213856.py:5: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.fillna(df.mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Valores nulos despu√©s de corregir infinitos:\n",
      "0\n",
      "\n",
      "### Normalizaci√≥n completada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ## 6Ô∏è‚É£ Normalizaci√≥n de Datos Num√©ricos\n",
    "\n",
    "# Reemplazamos valores infinitos con la media de la columna\n",
    "df.replace([float('inf'), float('-inf')], pd.NA, inplace=True)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Verificamos si a√∫n quedan valores infinitos o nulos despu√©s del reemplazo\n",
    "print(\"\\n### Valores nulos despu√©s de corregir infinitos:\")\n",
    "print(df.isnull().sum().sum())  # Deber√≠a ser 0\n",
    "\n",
    "# Aplicamos StandardScaler para normalizar los datos num√©ricos\n",
    "scaler = StandardScaler()\n",
    "columnas_numericas = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[columnas_numericas] = scaler.fit_transform(df[columnas_numericas])\n",
    "\n",
    "print(\"\\n### Normalizaci√≥n completada correctamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dataset limpio guardado en: archive/cleaned_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# ## 7Ô∏è‚É£ Guardar Dataset Limpio\n",
    "# Finalmente, guardamos el dataset limpio en un nuevo archivo CSV.\n",
    "cleaned_file_path = \"archive/cleaned_dataset.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "print(f\"\\n### Dataset limpio guardado en: {cleaned_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
